{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5611e9-10ee-4507-a697-0d799db161a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install google-cloud-pipeline-components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7de5bc8-353d-4f87-a145-e85fc005f7d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c864a318-630a-4815-8218-002da0b93b9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "PROJECT_ID=\"dt-tu-sandbox-dev\"\n",
    "BUCKET_NAME=\"gs://ovo-demos\"\n",
    "\n",
    "TRAIN_IMAGE = \"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8.py310:latest\" # Find the right images for tf training and deployment\n",
    "DEPLOY_IMAGE = \"europe-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest\"\n",
    "\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/cnn_pipeline_root/\"\n",
    "MODEL_URI = PIPELINE_ROOT + \"tf-model/\"\n",
    "\n",
    "shell_output = !gcloud auth list 2>/dev/null\n",
    "SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "EXPERIMENT_NAME = \"cnn-test-experiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8717319b-fde0-4ac5-821a-060c3dd7c597",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "\n",
    "from kfp import compiler, dsl\n",
    "from kfp.dsl import component, pipeline, Artifact, ClassificationMetrics, Input, Output, Model, Metrics\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from typing import NamedTuple\n",
    "\n",
    "from kfp.dsl import importer_node\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "from google_cloud_pipeline_components.v1.endpoint import (EndpointCreateOp,\n",
    "                                                          ModelDeployOp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d6ab5b-b824-4a9e-8cfb-ceae23b5d23a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(base_image=TRAIN_IMAGE, packages_to_install=[\"tensorflow_datasets==4.6.0\"])\n",
    "def custom_training_task(\n",
    "    model_uri: str,\n",
    "    epochs: int = 10,\n",
    "    batch_size: int = 32,\n",
    "):\n",
    "    import tensorflow_datasets as tfds\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.python.client import device_lib\n",
    "    import os\n",
    "    import sys\n",
    "    tfds.disable_progress_bar()\n",
    "    \n",
    "    gs_prefix = \"gs://\"\n",
    "    gcsfuse_prefix = \"/gcs/\"\n",
    "    model_path = model_uri.replace(gs_prefix, gcsfuse_prefix)\n",
    "\n",
    "    # Load the dataset\n",
    "    datasets, info = tfds.load('kmnist', with_info=True, as_supervised=True)\n",
    "\n",
    "    # Normalize and batch process the dataset\n",
    "    ds_train = datasets['train'].map(lambda x, y: (tf.cast(x, tf.float32)/255.0, y)).batch(batch_size)\n",
    "\n",
    "\n",
    "    # Build the Convolutional Neural Network\n",
    "    model = tf.keras.models.Sequential([                               \n",
    "          tf.keras.layers.Conv2D(16, (3,3), activation=tf.nn.relu, input_shape=(28, 28, 1), padding = \"same\"),\n",
    "          tf.keras.layers.MaxPooling2D(2,2),\n",
    "          tf.keras.layers.Conv2D(16, (3,3), activation=tf.nn.relu, padding = \"same\"),\n",
    "          tf.keras.layers.MaxPooling2D(2,2),\n",
    "          tf.keras.layers.Flatten(),\n",
    "          tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "          # TODO: Write the last layer.\n",
    "          # Hint: KMNIST has 10 output classes.\n",
    "          tf.keras.layers.Dense(10, activation=tf.nn.softmax),\n",
    "        ])\n",
    "\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "          loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "          metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "\n",
    "\n",
    "    # Train and save the model\n",
    "\n",
    "    model.fit(ds_train, epochs=epochs)\n",
    "\n",
    "    # TODO: Save your CNN classifier. \n",
    "    tf.saved_model.save(model,model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9d3b81-4407-4a5f-944e-ee1c6230c5b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@pipeline(\n",
    "    name=\"cnn-solution\",\n",
    "    description=\"cnn training pipeline\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "def pipeline(\n",
    "    model_display_name: str, \n",
    "    model_uri:str, \n",
    "    epochs: int, \n",
    "    batch_size: int,\n",
    "    model_deploy_compute: str\n",
    "):\n",
    "    training_task = custom_training_task(\n",
    "        model_uri=model_uri,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    import_unmanaged_model_task = importer_node.importer(\n",
    "        artifact_uri=model_uri,\n",
    "        artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "        metadata={\n",
    "            \"containerSpec\": {\n",
    "                \"imageUri\": DEPLOY_IMAGE \n",
    "            },\n",
    "        },\n",
    "    ).after(training_task)\n",
    "\n",
    "    model_upload_op = ModelUploadOp(\n",
    "        display_name=model_display_name,\n",
    "        unmanaged_container_model=import_unmanaged_model_task.outputs[\"artifact\"],\n",
    "    )\n",
    "\n",
    "    endpoint_create_op = EndpointCreateOp(\n",
    "        display_name=\"cnn-pipeline-created-endpoint\",\n",
    "    )\n",
    "    \n",
    "    _ = ModelDeployOp(\n",
    "        endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "        model=model_upload_op.outputs[\"model\"],\n",
    "        deployed_model_display_name=model_display_name,\n",
    "        dedicated_resources_machine_type=model_deploy_compute,\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82ff95a-27d4-462d-984e-d96c08b73d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"cnn-pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d1627f-1ba2-4041-b3aa-917f7fd0d61f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "    display_name=f\"cnn-pipeline-{TIMESTAMP}\",\n",
    "    template_path=\"cnn-pipeline.json\",\n",
    "    enable_caching=True,\n",
    "    parameter_values={\n",
    "        \"epochs\":1,\n",
    "        \"batch_size\":32,\n",
    "        \"model_uri\":MODEL_URI,\n",
    "        \"model_display_name\":f\"cnn-pipeline-model-{TIMESTAMPT}\",\n",
    "        \"model_deploy_compute\":\"n1-standard-4\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426c2e71-6fe4-470b-ab11-d120485cf6c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job.submit(service_account=SERVICE_ACCOUNT, experiment=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6302d2-9775-4760-b44a-b85042648d65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m114",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m114"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
